/*-------------------------------------------------------------------------
 *
 * communicator_new.c
 *	  Functions for communicating with remote pageservers.
 *
 * This is the "new" communicator. It consists of functions that
 * are called from the smgr implementation, in pagestore_smgr.c.
 *
 * Portions Copyright (c) 1996-2021, PostgreSQL Global Development Group
 * Portions Copyright (c) 1994, Regents of the University of California
 *
 *-------------------------------------------------------------------------
 */
#include "postgres.h"

#include <unistd.h>

#include "access/xlog.h"
#include "access/xlogdefs.h"
#if PG_VERSION_NUM >= 150000
#include "access/xlogrecovery.h"
#endif
#include "access/xlog_internal.h"
#include "access/xlogutils.h"
#include "common/hashfn.h"
#include "executor/instrument.h"
#include "miscadmin.h"
#include "postmaster/bgworker.h"
#include "postmaster/interrupt.h"
#include "postmaster/postmaster.h"
#include "replication/walsender.h"
#include "storage/fd.h"
#include "storage/ipc.h"
#include "storage/latch.h"
#include "storage/pmsignal.h"
#include "storage/procarray.h"
#if PG_VERSION_NUM >= 170000
#include "storage/procnumber.h"
#endif
#include "storage/spin.h"
#include "tcop/tcopprot.h"

#include "bitmap.h"
#include "communicator_new.h"
#include "communicator_process.h"
#include "hll.h"
#include "neon.h"
#include "neon_perf_counters.h"
#include "pagestore_client.h"

/*
 * FIXME: these are in file_cache.h, but I don't want to #include that
 * here. This code shouldn't be using the C file cache for anything else than
 * the GUCs.
 */
extern int	lfc_max_size;
extern int	lfc_size_limit;
extern char *lfc_path;


/* the rust bindings, generated by cbindgen */
#include "communicator/communicator_bindings.h"

#define MaxProcs (MaxBackends + NUM_AUXILIARY_PROCS)

static CommunicatorBackendStruct *my_bs;

static File cache_file = 0;

typedef struct CommunicatorShmemPerBackendData
{
	/*
	 * Latch used to notify backend of IO completion. We cannot use the
	 * standard process latch (MyProc->latch) because we cannot clear that
	 * latch as part of the IO handling, or we might cause the caller to miss
	 * some other events.
	 */
	Latch		io_completion_latch;

	/*
	 * Request counter, for assigning unique request IDs.
	 *
	 * This is only accessed by the backend itself, but we keep it in shared
	 * memory so that it survives across backend processes that are assigned
	 * the same proc number, to avoid reusing request IDs too fast.
	 */
	uint64		request_counter;

	/* Counters, for metrics. */
	uint64		cache_misses;
	uint64		cache_hits;

	/*
	 * Normally, when reading or writing pages from shared buffer cache, the
	 * worker process can operate directly on the shared buffer. But when
	 * working with a local buffer, we use this "bounce buffer" to pass the
	 * data to the worker process.
	 *
	 * TODO: That's slow, because it incurs an extra memory copy, and there's
	 * currently only one of these per backend, which means you can have only
	 * one such IO in progress at a time.
	 */
	PGIOAlignedBlock bounce_buffer;
} CommunicatorShmemPerBackendData;

typedef struct CommunicatorShmemData
{
	/*
	 * Estimation of working set size.
	 *
	 * Note that this is not protected by any locks. That's sloppy, but works
	 * fine in practice. To "add" a value to the HLL state, we just overwrite
	 * one of the timestamps. Calculating the estimate reads all the values,
	 * but it also doesn't depend on seeing a consistent snapshot of the
	 * values. We could get bogus results if accessing the TimestampTz was not
	 * atomic, but it on any 64-bit platforms we care about it is, and even if
	 * we observed a torn read every now and then, it wouldn't affect the
	 * overall estimate much.
	 */
	HyperLogLogState wss_estimation;

	CommunicatorShmemPerBackendData backends[]; /* MaxProcs */

	/* rust-managed shmem area follows at next MAXALIGN boundary */
} CommunicatorShmemData;

static CommunicatorShmemData *communicator_shmem_ptr;

static CommunicatorShmemPerBackendData *my_per_backend_data;
#define MyIOCompletionLatch (&my_per_backend_data->io_completion_latch)

#define MAX_INFLIGHT_ASYNC_REQUESTS 5

/* request indexes of (prefetch) requests that have been started */
static int	inflight_requests[MAX_INFLIGHT_ASYNC_REQUESTS];
static int	num_inflight_requests = 0;

static int	my_start_slot_idx;
static int	my_end_slot_idx;
static int	my_next_slot_idx;

static int	start_request(NeonIORequest *request, struct NeonIOResult *immediate_result_p);
static void wait_request_completion(int request_idx, struct NeonIOResult *result_p);
static void perform_request(NeonIORequest *request, struct NeonIOResult *result_p);
static void process_inflight_requests(void);

static bool bounce_needed(void *buffer);
static void *bounce_buf(void);
static void *bounce_write_if_needed(void *buffer);

static void communicator_new_backend_exit(int code, Datum arg);

static char *print_neon_io_request(NeonIORequest *request);

/*
 * Request ID assignment.
 *
 * Request IDs better be unique across all this compute's in-flight requests,
 * because they are used to match up responses to requests in the gRPC client
 * code. Furthermore, for logging and debugging purposes, it's nice to avoid
 * reusing them too fast, so that you can easily match up logs from different
 * components based on the request id.
 *
 * The request IDs we generate consist of two parts: the backend's ProcNumber
 * and a counter that can wrap-around.
 */
StaticAssertDecl(MAX_BACKENDS == 0x3FFFF, "Unexpected MAX_BACKENDS");
#define PROCNUMBER_BITS UINT64CONST(18)
#define REQUEST_COUNTER_BITS UINT64CONST(46)
#define REQUEST_COUNTER_MASK ((UINT64CONST(1) << REQUEST_COUNTER_BITS) - 1)

static inline uint64
assign_request_id(void)
{
	uint64		counter;
	uint64		result;

	counter = communicator_shmem_ptr->backends[MyProcNumber].request_counter++;
	result = (((uint64) MyProcNumber) << PROCNUMBER_BITS) | (counter & REQUEST_COUNTER_MASK);
	elog(DEBUG5, "assigned request id " UINT64_FORMAT " (counter " UINT64_FORMAT ", procno %d)", result, counter, (int) MyProcNumber);

	return result;
}

/**** Initialization functions. These run in postmaster ****/

static size_t
communicator_new_shmem_size(void)
{
	size_t		size = 0;
	int			num_request_slots;

	size += MAXALIGN(
					 offsetof(CommunicatorShmemData, backends) +
					 MaxProcs * sizeof(CommunicatorShmemPerBackendData)
		);

	num_request_slots = MaxProcs * MAX_INFLIGHT_ASYNC_REQUESTS;

	/* space needed by the rust code */
	size += rcommunicator_shmem_size(num_request_slots);

	return size;
}

void
CommunicatorNewShmemRequest(void)
{
	if (!neon_use_communicator_worker)
		return;
	if (neon_tenant[0] == '\0' || neon_timeline[0] == '\0')
		return;
	RequestAddinShmemSpace(communicator_new_shmem_size());
}

void
CommunicatorNewShmemInit(void)
{
	bool		found;
	int			pipefd[2];
	int			rc;
	size_t		communicator_size;
	size_t		shmem_size;
	void	   *shmem_ptr;
	uint64		initial_file_cache_size;
	uint64		max_file_cache_size;

	if (!neon_use_communicator_worker)
		return;

	if (neon_tenant[0] == '\0' || neon_timeline[0] == '\0')
	{
		elog(LOG, "disabling communicator worker because neon_tenant is empty");
		return;
	}

	rc = pipe(pipefd);
	if (rc != 0)
		ereport(ERROR,
				(errcode_for_file_access(),
				 errmsg_internal("could not create pipe between neon communicator and backends : %m")));
	if (fcntl(pipefd[0], F_SETFL, O_NONBLOCK) == -1)
		elog(FATAL, "fcntl(F_SETFL) failed on read-end of communicator pipe: %m");
	if (fcntl(pipefd[1], F_SETFL, O_NONBLOCK) == -1)
		elog(FATAL, "fcntl(F_SETFL) failed on write-end of communicator pipe: %m");

	shmem_size = communicator_new_shmem_size();
	shmem_ptr = ShmemInitStruct("Communicator shmem state",
								shmem_size,
								&found);
	Assert(!found);

	/* Initialize the C-managed parts */
	communicator_shmem_ptr = (CommunicatorShmemData *) shmem_ptr;
	communicator_size = MAXALIGN(offsetof(CommunicatorShmemData, backends) + MaxProcs * sizeof(CommunicatorShmemPerBackendData));
	shmem_ptr = (char *) shmem_ptr + communicator_size;
	shmem_size -= communicator_size;

	/* Initialize hyper-log-log structure for estimating working set size */
	initSHLL(&communicator_shmem_ptr->wss_estimation);

	for (int i = 0; i < MaxProcs; i++)
	{
		InitSharedLatch(&communicator_shmem_ptr->backends[i].io_completion_latch);
		communicator_shmem_ptr->backends[i].request_counter = 0;
	}

	/* lfc_size_limit is in MBs */
	initial_file_cache_size = lfc_size_limit * (1024 * 1024 / BLCKSZ);
	max_file_cache_size = lfc_max_size * (1024 * 1024 / BLCKSZ);
	if (initial_file_cache_size < 100)
		initial_file_cache_size = 100;
	if (max_file_cache_size < 100)
		max_file_cache_size = 100;

	/* Initialize the rust-managed parts */
	cis = rcommunicator_shmem_init(pipefd[0], pipefd[1], MaxProcs * MAX_INFLIGHT_ASYNC_REQUESTS, shmem_ptr, shmem_size,
								   initial_file_cache_size, max_file_cache_size);
}

/**** Worker process functions. These run in the communicator worker process ****/

/*
 * Callbacks from the rust code, in the communicator process.
 *
 * NOTE: These must be thread safe! It's very limited which PostgreSQL functions you can use!!!
 *
 * NOTE: the signatures of these better match the Rust definitions!
 */

void
notify_proc_unsafe(int procno)
{
	SetLatch(&communicator_shmem_ptr->backends[procno].io_completion_latch);
}

/**** Backend functions. These run in each backend ****/

/* Initialize per-backend private state */
void
communicator_new_init(void)
{
	Assert(cis != NULL);
	Assert(my_bs == NULL);

	if (pageserver_connstring[0] == '\0' && pageserver_grpc_urls[0] == '\0')
	{
		/* running with local storage */
		return;
	}

	/* The communicator process performs different initialization */
	if (MyBgworkerEntry && strcmp(MyBgworkerEntry->bgw_function_name, "communicator_new_bgworker_main") == 0)
		return;

	my_per_backend_data = &communicator_shmem_ptr->backends[MyProcNumber];

	OwnLatch(MyIOCompletionLatch);

	my_bs = rcommunicator_backend_init(cis, MyProcNumber);
	cis = NULL;

	/*
	 * Check the status of all the request slots. A previous backend with the
	 * same proc number might've left behind some prefetch requests or aborted
	 * requests
	 */
	my_start_slot_idx = MyProcNumber * MAX_INFLIGHT_ASYNC_REQUESTS;
	my_end_slot_idx = my_start_slot_idx + MAX_INFLIGHT_ASYNC_REQUESTS;
	my_next_slot_idx = my_start_slot_idx;

	for (int idx = my_start_slot_idx; idx < my_end_slot_idx; idx++)
	{
		struct NeonIOResult result;

		if (bcomm_get_request_slot_status(my_bs, idx))
		{
			elog(LOG, "processing leftover IO request from previous session at slot %d", idx);
			wait_request_completion(idx, &result);

			/* FIXME: log the result if it was an error */
		}
	}

	/*
	 * Arrange to clean up at backend exit.
	 */
	on_shmem_exit(communicator_new_backend_exit, 0);
}

static void
communicator_new_backend_exit(int code, Datum arg)
{
	DisownLatch(MyIOCompletionLatch);
}

/*
 * prefetch_register_bufferv() - register and prefetch buffers
 *
 * Register that we may want the contents of BufferTag in the near future.
 * This is used when issuing a speculative prefetch request, but also when
 * performing a synchronous request and need the buffer right now.
 *
 * When performing a prefetch rather than a synchronous request,
 * is_prefetch==true. Currently, it only affects how the request is accounted
 * in the perf counters.
 *
 * NOTE: this function may indirectly update MyPState->pfs_hash; which
 * invalidates any active pointers into the hash table.
 */
void
communicator_new_prefetch_register_bufferv(NRelFileInfo rinfo, ForkNumber forkNum,
										   BlockNumber blockno, BlockNumber nblocks)
{
	NeonIORequest request = {
		.tag = NeonIORequest_PrefetchV,
		.prefetch_v = {
			.request_id = assign_request_id(),
			.spc_oid = NInfoGetSpcOid(rinfo),
			.db_oid = NInfoGetDbOid(rinfo),
			.rel_number = NInfoGetRelNumber(rinfo),
			.fork_number = forkNum,
			.block_number = blockno,
			.nblocks = nblocks,
		}
	};
	struct NeonIOResult result;

	elog(DEBUG5, "prefetch called for rel %u/%u/%u.%u block %u (%u blocks)",
		 RelFileInfoFmt(rinfo), forkNum, blockno, nblocks);

	if (num_inflight_requests >= MAX_INFLIGHT_ASYNC_REQUESTS)
		process_inflight_requests();

	/* Fire and forget the request */
	(void) start_request(&request, &result);
}

/*
 * Check if LFC contains the given buffer, and update its last-written LSN if
 * not.
 *
 * This is used in WAL replay in read replica, to skip updating pages that are
 * not in cache.
 */
bool
communicator_new_update_lwlsn_for_block_if_not_cached(NRelFileInfo rinfo, ForkNumber forkNum,
													  BlockNumber blockno, XLogRecPtr lsn)
{
	return bcomm_update_lw_lsn_for_block_if_not_cached(my_bs,
													   NInfoGetSpcOid(rinfo),
													   NInfoGetDbOid(rinfo),
													   NInfoGetRelNumber(rinfo),
													   forkNum,
													   blockno,
													   lsn);
}

/* Dump a list of blocks in the LFC, for use in prewarming later */
FileCacheState *
communicator_new_get_lfc_state(size_t max_entries)
{
	struct FileCacheIterator iter;
	FileCacheState *fcs;
	uint8	   *bitmap;

	/* TODO: Max(max_entries, <current # of entries in cache>) */
	size_t		n_entries = max_entries;
	size_t		state_size = FILE_CACHE_STATE_SIZE_FOR_CHUNKS(n_entries, 1);
	size_t		n_pages = 0;

	fcs = (FileCacheState *) palloc0(state_size);
	SET_VARSIZE(fcs, state_size);
	fcs->magic = FILE_CACHE_STATE_MAGIC;
	fcs->chunk_size_log = 0;
	fcs->n_chunks = n_entries;
	bitmap = FILE_CACHE_STATE_BITMAP(fcs);

	bcomm_cache_iterate_begin(my_bs, &iter);
	while (n_pages < max_entries && bcomm_cache_iterate_next(my_bs, &iter))
	{
		BufferTag	tag;

		BufTagInit(tag, iter.rel_number, iter.fork_number, iter.block_number, iter.spc_oid, iter.db_oid);
		fcs->chunks[n_pages] = tag;
		n_pages++;
	}

	/*
	 * fill bitmap. TODO: memset would be more efficient, but this is a silly
	 * format anyway
	 */
	for (size_t i = 0; i < n_pages; i++)
	{
		BITMAP_SET(bitmap, i);
	}
	fcs->n_pages = n_pages;

	return fcs;
}

/*
 * Drain all in-flight requests from the queue.
 *
 * This is used to drain prefetch requests that have been acknowledged by the
 * communicator, before we perform a synchronous request.  (With Postgres v18
 * and async IO, managing the in-flight requests will get more complicated,
 * but this will do for now.)
 *
 * We can also have some an in-flight request queued up, if the query is
 * cancelled while a synchronous request is being processed, in
 * wait_request_completion().
 */
static void
process_inflight_requests(void)
{
	struct NeonIOResult result;

	/* FIXME: log errors */
	for (int i = 0; i < num_inflight_requests; i++)
	{
		elog(DEBUG4, "processing prefetch request with idx %d", inflight_requests[i]);
		wait_request_completion(inflight_requests[i], &result);
	}
	num_inflight_requests = 0;
}

/*
 * Perform an IO request in a synchronous fashion.
 *
 * Returns a pointer to the result slot. It is valid until the next time a
 * request is submitted.
 */
static void
perform_request(NeonIORequest *request, struct NeonIOResult *result_p)
{
	int			request_idx;

	process_inflight_requests();

	request_idx = start_request(request, result_p);
	if (request_idx == -1)
	{
		/* it was completed immediately */
		return;
	}
	wait_request_completion(request_idx, result_p);
	Assert(num_inflight_requests == 1);
	Assert(inflight_requests[0] == request_idx);
	num_inflight_requests = 0;
}

static int
start_request(NeonIORequest *request, struct NeonIOResult *immediate_result_p)
{
	int			request_idx;

	Assert(num_inflight_requests < MAX_INFLIGHT_ASYNC_REQUESTS);

	request_idx = bcomm_start_io_request(my_bs, my_next_slot_idx, request, immediate_result_p);
	if (request_idx == -1)
	{
		/* -1 means the request was satisfied immediately. */
		elog(DEBUG4, "communicator request %s was satisfied immediately", print_neon_io_request(request));
		return -1;
	}
	Assert(request_idx == my_next_slot_idx);
	my_next_slot_idx++;
	if (my_next_slot_idx == my_end_slot_idx)
		my_next_slot_idx = my_start_slot_idx;

	inflight_requests[num_inflight_requests] = request_idx;
	num_inflight_requests++;

	elog(DEBUG5, "started communicator request %s at slot %d", print_neon_io_request(request), request_idx);

	return request_idx;
}

static void
wait_request_completion(int request_idx, struct NeonIOResult *result_p)
{
	int32_t		poll_res;
	TimestampTz start_time;

	/* fixme: check 'request_idx' ? */

	start_time = GetCurrentTimestamp();
	for (;;)
	{
		TimestampTz now;

		ResetLatch(MyIOCompletionLatch);

		poll_res = bcomm_poll_request_completion(my_bs, request_idx, result_p);
		if (poll_res == -1)
		{
			/*
			 * Wake up periodically for CHECK_FOR_INTERRUPTS(). Because we
			 * wait on MyIOCompletionLatch rather than MyLatch, we won't be
			 * woken up for the standard interrupts.
			 */
			long		timeout_ms = 1000;

			/*
			 * If the query is cancelled, we will bail out here, and leave the
			 * in-flight request in the request queue. It will be waited for
			 * again and processed when the next request is issued, in
			 * process_inflight_requests().
			 */
			CHECK_FOR_INTERRUPTS();

			/*
			 * FIXME: as a temporary hack, panic if we don't get a response
			 * promptly. Lots of regression tests are getting stuck and
			 * failing at the moment, this makes them fail a little faster,
			 * which it faster to iterate. This needs to be removed once more
			 * regression tests are passing.
			 */
			now = GetCurrentTimestamp();
			if (now - start_time > 120 * 1000 * 1000)
			{
				elog(PANIC, "timed out waiting for response from communicator process at slot %d", request_idx);
			}

			(void) WaitLatch(MyIOCompletionLatch,
							 WL_EXIT_ON_PM_DEATH | WL_LATCH_SET | WL_TIMEOUT,
							 timeout_ms,
							 WAIT_EVENT_NEON_PS_STARTING);
			continue;			/* still busy */
		}
		else if (poll_res == 0)
		{
			return;
		}
		else
		{
			elog(ERROR, "unexpected return code from bcomm_poll_request_completion()");
		}
	}
}

/*
 *	Does the physical file exist?
 */
bool
communicator_new_rel_exists(NRelFileInfo rinfo, ForkNumber forkNum)
{
	NeonIORequest request = {
		.tag = NeonIORequest_RelSize,
		.rel_size = {
			.request_id = assign_request_id(),
			.spc_oid = NInfoGetSpcOid(rinfo),
			.db_oid = NInfoGetDbOid(rinfo),
			.rel_number = NInfoGetRelNumber(rinfo),
			.fork_number = forkNum,
			.allow_missing = true,
		}
	};
	NeonIOResult result;

	perform_request(&request, &result);
	switch (result.tag)
	{
		case NeonIOResult_RelSize:
			return result.rel_size != InvalidBlockNumber;
		case NeonIOResult_Error:
			errno = result.error;
			ereport(ERROR,
					(errcode_for_file_access(),
					 errmsg("could not check existence of rel %u/%u/%u.%u: %m",
							RelFileInfoFmt(rinfo), forkNum)));
			break;
		default:
			elog(ERROR, "unexpected result for RelSize operation: %d", result.tag);
			break;
	}
}

/*
 * Read N consecutive pages from a relation
 */
void
communicator_new_readv(NRelFileInfo rinfo, ForkNumber forkNum, BlockNumber blockno,
					   void **buffers, BlockNumber nblocks)
{
	NeonIOResult result;
	CCachedGetPageVResult cached_result;
	void	   *bounce_buf_used = NULL;
	int			request_idx;
	NeonIORequest request = {
		.tag = NeonIORequest_GetPageV,
		.get_page_v = {
			.request_id = assign_request_id(),
			.spc_oid = NInfoGetSpcOid(rinfo),
			.db_oid = NInfoGetDbOid(rinfo),
			.rel_number = NInfoGetRelNumber(rinfo),
			.fork_number = forkNum,
			.block_number = blockno,
			.nblocks = nblocks,
		}
	};

	{
		BufferTag	tag;

		CopyNRelFileInfoToBufTag(tag, rinfo);
		tag.forkNum = forkNum;
		for (int i = 0; i < nblocks; i++)
		{
			tag.blockNum = blockno;
			addSHLL(&communicator_shmem_ptr->wss_estimation,
					hash_bytes((uint8_t *) & tag, sizeof(tag)));
		}
	}

	elog(DEBUG5, "getpagev called for rel %u/%u/%u.%u block %u (%u blocks)",
		 RelFileInfoFmt(rinfo), forkNum, blockno, nblocks);

	/*
	 * Fill in the destination buffer pointers in the request. If the
	 * destination is a buffer in shared memory, the communicator process can
	 * write the result directly to the buffer. Otherwise, we need to use a
	 * "bounce buffer". We only have one bounce buffer, so if bouncing is
	 * needed and multiple pages were requested, we need to serially perform a
	 * separate request for each page. Hopefully that is rare.
	 */
	if (nblocks == 1)
	{
		if (bounce_needed(buffers[0]))
		{
			bounce_buf_used = bounce_buf();
			request.get_page_v.dest[0].ptr = bounce_buf_used;
		}
		else
			request.get_page_v.dest[0].ptr = buffers[0];
	}
	else
	{
		for (int i = 0; i < nblocks; i++)
		{
			if (bounce_needed(buffers[i]))
			{
				/* Split the vector-request into single page requests */
				for (int j = 0; j < nblocks; j++)
				{
					communicator_new_readv(rinfo, forkNum, blockno + j,
										   &buffers[j], 1);
				}
				return;
			}
			request.get_page_v.dest[i].ptr = buffers[i];
		}
	}

	process_inflight_requests();

retry:
	request_idx = bcomm_start_get_page_v_request(my_bs, my_next_slot_idx, &request, &cached_result);
	if (request_idx == -1)
	{
		bool		completed;

		/*
		 * LFC hit, but we are responsible for completing the I/O on the local
		 * file
		 */
		if (cache_file == 0)
			cache_file = PathNameOpenFile(lfc_path, O_RDONLY | PG_BINARY);

		for (int i = 0; i < nblocks; i++)
		{
			uint64_t	cached_block = cached_result.cache_block_numbers[i];
			char	   *buffer = buffers[i];
			ssize_t		bytes_total = 0;

			while (bytes_total < BLCKSZ)
			{
				ssize_t		nbytes;

				nbytes = FileRead(cache_file, buffer + bytes_total, BLCKSZ - bytes_total, cached_block * BLCKSZ + bytes_total, WAIT_EVENT_NEON_LFC_READ);
				if (nbytes == -1)
					ereport(ERROR,
							(errcode_for_file_access(),
							 errmsg("could not read block %lu in local cache file: %m",
									cached_block)));
				if (nbytes == 0)
				{
					/*
					 * FIXME: if the file was concurrently truncated, I guess
					 * this is expected. We should finish the read by calling
					 * bcomm_finish_cache_read(), and only throw the error if
					 * it reported success.
					 */
					ereport(ERROR,
							(errcode_for_file_access(),
							 errmsg("could not read block %lu in local cache file (unexpected EOF)",
									cached_block)));
				}
				bytes_total += nbytes;
			}
		}
		completed = bcomm_finish_cache_read(my_bs);
		if (!completed)
		{
			elog(DEBUG1, "read from local cache file was superseded by concurrent update");
			goto retry;
		}

		pgBufferUsage.file_cache.hits += nblocks;
		my_per_backend_data->cache_hits += nblocks;

		return;
	}
	Assert(request_idx == my_next_slot_idx);
	my_next_slot_idx++;
	if (my_next_slot_idx == my_end_slot_idx)
		my_next_slot_idx = my_start_slot_idx;
	inflight_requests[num_inflight_requests] = request_idx;
	num_inflight_requests++;

	/*
	 * XXX: If some blocks were in cache but not others, we count all blocks
	 * as a cache miss.
	 */
	pgBufferUsage.file_cache.misses += nblocks;
	my_per_backend_data->cache_misses += nblocks;

	wait_request_completion(request_idx, &result);
	Assert(num_inflight_requests == 1);
	Assert(inflight_requests[0] == request_idx);
	num_inflight_requests = 0;
	switch (result.tag)
	{
		case NeonIOResult_GetPageV:
			if (bounce_buf_used)
				memcpy(buffers[0], bounce_buf_used, BLCKSZ);
			return;
		case NeonIOResult_Error:
			errno = result.error;
			if (nblocks > 0)
				ereport(ERROR,
						(errcode_for_file_access(),
						 errmsg("could not read block %u in rel %u/%u/%u.%u: %m",
								blockno, RelFileInfoFmt(rinfo), forkNum)));
			else
				ereport(ERROR,
						(errcode_for_file_access(),
						 errmsg("could not read %u blocks at %u in rel %u/%u/%u.%u: %m",
								nblocks, blockno, RelFileInfoFmt(rinfo), forkNum)));
			break;
		default:
			elog(ERROR, "unexpected result for GetPageV operation: %d", result.tag);
			break;
	}
}

/*
 * Read a page at given LSN, bypassing the LFC.
 *
 * For tests and debugging purposes only.
 */
void
communicator_new_read_at_lsn_uncached(NRelFileInfo rinfo, ForkNumber forkNum, BlockNumber blockno,
									  void *buffer, XLogRecPtr request_lsn, XLogRecPtr not_modified_since)
{
	NeonIOResult result;
	void	   *bounce_buf_used;
	NeonIORequest request = {
		.tag = NeonIORequest_GetPageVUncached,
		.get_page_v_uncached = {
			.request_id = assign_request_id(),
			.spc_oid = NInfoGetSpcOid(rinfo),
			.db_oid = NInfoGetDbOid(rinfo),
			.rel_number = NInfoGetRelNumber(rinfo),
			.fork_number = forkNum,
			.block_number = blockno,
			.nblocks = 1,
			.request_lsn = request_lsn,
			.not_modified_since = not_modified_since,
		}
	};

	/*
	 * This is for tests only and doesn't need to be particularly fast. Always
	 * use the bounce buffer for simplicity
	 */
	request.get_page_v_uncached.dest[0].ptr = bounce_buf_used = bounce_buf();

	/*
	 * don't use the specialized bcomm_start_get_page_v_request() function
	 * here, because we want to bypass the LFC
	 */
	perform_request(&request, &result);
	switch (result.tag)
	{
		case NeonIOResult_GetPageV:
			memcpy(buffer, bounce_buf_used, BLCKSZ);
			return;
		case NeonIOResult_Error:
			errno = result.error;
			ereport(ERROR,
					(errcode_for_file_access(),
					 errmsg("could not read (uncached) block %u in rel %u/%u/%u.%u: %m",
							blockno, RelFileInfoFmt(rinfo), forkNum)));
			break;
		default:
			elog(ERROR, "unexpected result for GetPageV operation: %d", result.tag);
			break;
	}
}

/*
 *	neon_nblocks() -- Get the number of blocks stored in a relation.
 */
BlockNumber
communicator_new_rel_nblocks(NRelFileInfo rinfo, ForkNumber forkNum)
{
	NeonIORequest request = {
		.tag = NeonIORequest_RelSize,
		.rel_size = {
			.request_id = assign_request_id(),
			.spc_oid = NInfoGetSpcOid(rinfo),
			.db_oid = NInfoGetDbOid(rinfo),
			.rel_number = NInfoGetRelNumber(rinfo),
			.fork_number = forkNum,
			.allow_missing = false,
		}
	};
	NeonIOResult result;

	perform_request(&request, &result);
	switch (result.tag)
	{
		case NeonIOResult_RelSize:
			return result.rel_size;
		case NeonIOResult_Error:
			errno = result.error;
			ereport(ERROR,
					(errcode_for_file_access(),
					 errmsg("could not read size of rel %u/%u/%u.%u: %m",
							RelFileInfoFmt(rinfo), forkNum)));
			break;
		default:
			elog(ERROR, "unexpected result for RelSize operation: %d", result.tag);
			break;
	}
}

/*
 *	neon_db_size() -- Get the size of the database in bytes.
 */
int64
communicator_new_dbsize(Oid dbNode)
{
	NeonIORequest request = {
		.tag = NeonIORequest_DbSize,
		.db_size = {
			.request_id = assign_request_id(),
			.db_oid = dbNode,
		}
	};
	NeonIOResult result;

	perform_request(&request, &result);
	switch (result.tag)
	{
		case NeonIOResult_DbSize:
			return (int64) result.db_size;
		case NeonIOResult_Error:
			errno = result.error;
			ereport(ERROR,
					(errcode_for_file_access(),
					 errmsg("could not read database size of database %u: %m",
							dbNode)));
			break;
		default:
			elog(ERROR, "unexpected result for DbSize operation: %d", result.tag);
			break;
	}
}

int
communicator_new_read_slru_segment(
								   SlruKind kind,
								   uint32_t segno,
								   neon_request_lsns * request_lsns,
								   const char *path)
{
	NeonIOResult result = {};
	NeonIORequest request = {
		.tag = NeonIORequest_ReadSlruSegment,
		.read_slru_segment = {
			.request_id = assign_request_id(),
			.slru_kind = kind,
			.segment_number = segno,
			.request_lsn = request_lsns->request_lsn,
		}
	};
	int			nblocks = -1;
	char	   *temp_path = bounce_buf();

	if (path == NULL)
	{
		elog(ERROR, "read_slru_segment called with NULL path");
		return -1;
	}

	strlcpy(temp_path, path, BLCKSZ);
	request.read_slru_segment.destination_file_path.ptr = (uint8_t *) temp_path;

	elog(DEBUG5, "readslrusegment called for kind=%u, segno=%u, file_path=\"%s\"",
		 kind, segno, request.read_slru_segment.destination_file_path.ptr);

	/* FIXME: see `request_lsns` in main_loop.rs for why this is needed */
	XLogSetAsyncXactLSN(request_lsns->request_lsn);

	perform_request(&request, &result);

	switch (result.tag)
	{
		case NeonIOResult_ReadSlruSegment:
			nblocks = result.read_slru_segment;
			break;
		case NeonIOResult_Error:
			errno = result.error;
			ereport(ERROR,
					(errcode_for_file_access(),
					 errmsg("could not read slru segment, kind=%u, segno=%u: %m",
							kind, segno)));
			break;
		default:
			elog(ERROR, "unexpected result for read SLRU operation: %d", result.tag);
			break;
	}

	return nblocks;
}

/* Write requests */
void
communicator_new_write_page(NRelFileInfo rinfo, ForkNumber forkNum, BlockNumber blockno,
							const void *buffer, XLogRecPtr lsn)
{
	void	   *src = bounce_write_if_needed((void *) buffer);
	NeonIORequest request = {
		.tag = NeonIORequest_WritePage,
		.write_page = {
			.request_id = assign_request_id(),
			.spc_oid = NInfoGetSpcOid(rinfo),
			.db_oid = NInfoGetDbOid(rinfo),
			.rel_number = NInfoGetRelNumber(rinfo),
			.fork_number = forkNum,
			.block_number = blockno,
			.lsn = lsn,
			.src.ptr = src,
		}
	};
	NeonIOResult result;

	/* FIXME: see `request_lsns` in main_loop.rs for why this is needed */
	XLogSetAsyncXactLSN(lsn);

	perform_request(&request, &result);
	switch (result.tag)
	{
		case NeonIOResult_WriteOK:
			return;
		case NeonIOResult_Error:
			errno = result.error;
			ereport(ERROR,
					(errcode_for_file_access(),
					 errmsg("could not write block %u in rel %u/%u/%u.%u: %m",
							blockno, RelFileInfoFmt(rinfo), forkNum)));
			break;
		default:
			elog(ERROR, "unexpected result for WritePage operation: %d", result.tag);
			break;
	}
}

void
communicator_new_rel_extend(NRelFileInfo rinfo, ForkNumber forkNum, BlockNumber blockno,
							const void *buffer, XLogRecPtr lsn)
{
	void	   *src = bounce_write_if_needed((void *) buffer);
	NeonIORequest request = {
		.tag = NeonIORequest_RelExtend,
		.rel_extend = {
			.request_id = assign_request_id(),
			.spc_oid = NInfoGetSpcOid(rinfo),
			.db_oid = NInfoGetDbOid(rinfo),
			.rel_number = NInfoGetRelNumber(rinfo),
			.fork_number = forkNum,
			.block_number = blockno,
			.lsn = lsn,
			.src.ptr = src,
		}
	};
	NeonIOResult result;

	/* FIXME: see `request_lsns` in main_loop.rs for why this is needed */
	XLogSetAsyncXactLSN(lsn);

	perform_request(&request, &result);
	switch (result.tag)
	{
		case NeonIOResult_WriteOK:
			return;
		case NeonIOResult_Error:
			errno = result.error;
			ereport(ERROR,
					(errcode_for_file_access(),
					 errmsg("could not extend to block %u in rel %u/%u/%u.%u: %m",
							blockno, RelFileInfoFmt(rinfo), forkNum)));
			break;
		default:
			elog(ERROR, "unexpected result for Extend operation: %d", result.tag);
			break;
	}
}

void
communicator_new_rel_zeroextend(NRelFileInfo rinfo, ForkNumber forkNum, BlockNumber blockno,
								BlockNumber nblocks, XLogRecPtr lsn)
{
	NeonIORequest request = {
		.tag = NeonIORequest_RelZeroExtend,
		.rel_zero_extend = {
			.request_id = assign_request_id(),
			.spc_oid = NInfoGetSpcOid(rinfo),
			.db_oid = NInfoGetDbOid(rinfo),
			.rel_number = NInfoGetRelNumber(rinfo),
			.fork_number = forkNum,
			.block_number = blockno,
			.nblocks = nblocks,
			.lsn = lsn,
		}
	};
	NeonIOResult result;

	/* FIXME: see `request_lsns` in main_loop.rs for why this is needed */
	XLogSetAsyncXactLSN(lsn);

	perform_request(&request, &result);
	switch (result.tag)
	{
		case NeonIOResult_WriteOK:
			return;
		case NeonIOResult_Error:
			errno = result.error;
			ereport(ERROR,
					(errcode_for_file_access(),
					 errmsg("could not zeroextend to block %u in rel %u/%u/%u.%u: %m",
							blockno, RelFileInfoFmt(rinfo), forkNum)));
			break;
		default:
			elog(ERROR, "unexpected result for ZeroExtend operation: %d", result.tag);
			break;
	}
}

void
communicator_new_rel_create(NRelFileInfo rinfo, ForkNumber forkNum, XLogRecPtr lsn)
{
	NeonIORequest request = {
		.tag = NeonIORequest_RelCreate,
		.rel_create = {
			.request_id = assign_request_id(),
			.spc_oid = NInfoGetSpcOid(rinfo),
			.db_oid = NInfoGetDbOid(rinfo),
			.rel_number = NInfoGetRelNumber(rinfo),
			.fork_number = forkNum,
			.lsn = lsn,
		}
	};
	NeonIOResult result;

	/* FIXME: see `request_lsns` in main_loop.rs for why this is needed */
	XLogSetAsyncXactLSN(lsn);

	perform_request(&request, &result);
	switch (result.tag)
	{
		case NeonIOResult_WriteOK:
			return;
		case NeonIOResult_Error:
			errno = result.error;
			ereport(ERROR,
					(errcode_for_file_access(),
					 errmsg("could not create rel %u/%u/%u.%u: %m",
							RelFileInfoFmt(rinfo), forkNum)));
			break;
		default:
			elog(ERROR, "unexpected result for Create operation: %d", result.tag);
			break;
	}
}

void
communicator_new_rel_truncate(NRelFileInfo rinfo, ForkNumber forkNum, BlockNumber nblocks, XLogRecPtr lsn)
{
	NeonIORequest request = {
		.tag = NeonIORequest_RelTruncate,
		.rel_truncate = {
			.request_id = assign_request_id(),
			.spc_oid = NInfoGetSpcOid(rinfo),
			.db_oid = NInfoGetDbOid(rinfo),
			.rel_number = NInfoGetRelNumber(rinfo),
			.fork_number = forkNum,
			.nblocks = nblocks,
			.lsn = lsn,
		}
	};
	NeonIOResult result;

	/* FIXME: see `request_lsns` in main_loop.rs for why this is needed */
	XLogSetAsyncXactLSN(lsn);

	perform_request(&request, &result);
	switch (result.tag)
	{
		case NeonIOResult_WriteOK:
			return;
		case NeonIOResult_Error:
			errno = result.error;
			ereport(ERROR,
					(errcode_for_file_access(),
					 errmsg("could not truncate rel %u/%u/%u.%u to %u blocks: %m",
							RelFileInfoFmt(rinfo), forkNum, nblocks)));
			break;
		default:
			elog(ERROR, "unexpected result for Truncate operation: %d", result.tag);
			break;
	}
}

void
communicator_new_rel_unlink(NRelFileInfo rinfo, ForkNumber forkNum, XLogRecPtr lsn)
{
	NeonIORequest request = {
		.tag = NeonIORequest_RelUnlink,
		.rel_unlink = {
			.request_id = assign_request_id(),
			.spc_oid = NInfoGetSpcOid(rinfo),
			.db_oid = NInfoGetDbOid(rinfo),
			.rel_number = NInfoGetRelNumber(rinfo),
			.fork_number = forkNum,
			.lsn = lsn,
		}
	};
	NeonIOResult result;

	/* FIXME: see `request_lsns` in main_loop.rs for why this is needed */
	XLogSetAsyncXactLSN(lsn);

	perform_request(&request, &result);
	switch (result.tag)
	{
		case NeonIOResult_WriteOK:
			return;
		case NeonIOResult_Error:
			errno = result.error;
			ereport(ERROR,
					(errcode_for_file_access(),
					 errmsg("could not unlink rel %u/%u/%u.%u: %m",
							RelFileInfoFmt(rinfo), forkNum)));
			break;
		default:
			elog(ERROR, "unexpected result for Unlink operation: %d", result.tag);
			break;
	}
}

void
communicator_new_update_cached_rel_size(NRelFileInfo rinfo, ForkNumber forkNum, BlockNumber nblocks, XLogRecPtr lsn)
{
	NeonIORequest request = {
		.tag = NeonIORequest_UpdateCachedRelSize,
		.update_cached_rel_size = {
			.request_id = assign_request_id(),
			.spc_oid = NInfoGetSpcOid(rinfo),
			.db_oid = NInfoGetDbOid(rinfo),
			.rel_number = NInfoGetRelNumber(rinfo),
			.fork_number = forkNum,
			.nblocks = nblocks,
			.lsn = lsn,
		}
	};
	NeonIOResult result;

	perform_request(&request, &result);
	switch (result.tag)
	{
		case NeonIOResult_WriteOK:
			return;
		case NeonIOResult_Error:
			errno = result.error;
			ereport(ERROR,
					(errcode_for_file_access(),
					 errmsg("could not update cached size for rel %u/%u/%u.%u: %m",
							RelFileInfoFmt(rinfo), forkNum)));
			break;
		default:
			elog(ERROR, "unexpected result for UpdateCachedRelSize operation: %d", result.tag);
			break;
	}
}

/* Debugging functions */

static char *
print_neon_io_request(NeonIORequest *request)
{
	static char buf[100];

	switch (request->tag)
	{
		case NeonIORequest_Empty:
			snprintf(buf, sizeof(buf), "Empty");
			return buf;
		case NeonIORequest_RelSize:
			{
				CRelSizeRequest *r = &request->rel_size;

				snprintf(buf, sizeof(buf), "RelSize: req " UINT64_FORMAT " rel %u/%u/%u.%u allow_missing: %d",
						 r->request_id,
						 r->spc_oid, r->db_oid, r->rel_number, r->fork_number,
						 r->allow_missing);
				return buf;
			}
		case NeonIORequest_GetPageV:
			{
				CGetPageVRequest *r = &request->get_page_v;

				snprintf(buf, sizeof(buf), "GetPageV: req " UINT64_FORMAT " rel %u/%u/%u.%u blks %d-%d",
						 r->request_id,
						 r->spc_oid, r->db_oid, r->rel_number, r->fork_number, r->block_number, r->block_number + r->nblocks);
				return buf;
			}
		case NeonIORequest_GetPageVUncached:
			{
				CGetPageVUncachedRequest *r = &request->get_page_v_uncached;

				snprintf(buf, sizeof(buf), "GetPageVUncached: req " UINT64_FORMAT " rel %u/%u/%u.%u blk %d request_lsn %X/%X not_modified_since %X/%X",
						 r->request_id,
						 r->spc_oid, r->db_oid, r->rel_number, r->fork_number, r->block_number,
						 LSN_FORMAT_ARGS(r->request_lsn), LSN_FORMAT_ARGS(r->not_modified_since));
				return buf;
			}
		case NeonIORequest_ReadSlruSegment:
			{
				CReadSlruSegmentRequest *r = &request->read_slru_segment;

				snprintf(buf, sizeof(buf), "ReadSlruSegment: req " UINT64_FORMAT " slrukind=%u, segno=%u, lsn=%X/%X, file_path=\"%s\"",
						 r->request_id,
						 r->slru_kind,
						 r->segment_number,
						 LSN_FORMAT_ARGS(r->request_lsn),
						 r->destination_file_path.ptr);
				return buf;
			}
		case NeonIORequest_PrefetchV:
			{
				CPrefetchVRequest *r = &request->prefetch_v;

				snprintf(buf, sizeof(buf), "PrefetchV: req " UINT64_FORMAT " rel %u/%u/%u.%u blks %d-%d",
						 r->request_id,
						 r->spc_oid, r->db_oid, r->rel_number, r->fork_number, r->block_number, r->block_number + r->nblocks);
				return buf;
			}
		case NeonIORequest_DbSize:
			{
				CDbSizeRequest *r = &request->db_size;

				snprintf(buf, sizeof(buf), "PrefetchV: req " UINT64_FORMAT " db %u",
						 r->request_id, r->db_oid);
				return buf;
			}
		case NeonIORequest_WritePage:
			{
				CWritePageRequest *r = &request->write_page;

				snprintf(buf, sizeof(buf), "WritePage: req " UINT64_FORMAT " rel %u/%u/%u.%u blk %u lsn %X/%X",
						 r->request_id,
						 r->spc_oid, r->db_oid, r->rel_number, r->fork_number, r->block_number,
						 LSN_FORMAT_ARGS(r->lsn));
				return buf;
			}
		case NeonIORequest_RelExtend:
			{
				CRelExtendRequest *r = &request->rel_extend;

				snprintf(buf, sizeof(buf), "RelExtend: req " UINT64_FORMAT " rel %u/%u/%u.%u blk %u lsn %X/%X",
						 r->request_id,
						 r->spc_oid, r->db_oid, r->rel_number, r->fork_number, r->block_number,
						 LSN_FORMAT_ARGS(r->lsn));
				return buf;
			}
		case NeonIORequest_RelZeroExtend:
			{
				CRelZeroExtendRequest *r = &request->rel_zero_extend;

				snprintf(buf, sizeof(buf), "RelZeroExtend: req " UINT64_FORMAT " rel %u/%u/%u.%u blks %u-%u lsn %X/%X",
						 r->request_id,
						 r->spc_oid, r->db_oid, r->rel_number, r->fork_number, r->block_number, r->block_number + r->nblocks,
						 LSN_FORMAT_ARGS(r->lsn));
				return buf;
			}
		case NeonIORequest_RelCreate:
			{
				CRelCreateRequest *r = &request->rel_create;

				snprintf(buf, sizeof(buf), "RelCreate: req " UINT64_FORMAT " rel %u/%u/%u.%u",
						 r->request_id,
						 r->spc_oid, r->db_oid, r->rel_number, r->fork_number);
				return buf;
			}
		case NeonIORequest_RelTruncate:
			{
				CRelTruncateRequest *r = &request->rel_truncate;

				snprintf(buf, sizeof(buf), "RelTruncate: req " UINT64_FORMAT " rel %u/%u/%u.%u blks %u",
						 r->request_id,
						 r->spc_oid, r->db_oid, r->rel_number, r->fork_number, r->nblocks);
				return buf;
			}
		case NeonIORequest_RelUnlink:
			{
				CRelUnlinkRequest *r = &request->rel_unlink;

				snprintf(buf, sizeof(buf), "RelUnlink: req " UINT64_FORMAT " rel %u/%u/%u.%u",
						 r->request_id,
						 r->spc_oid, r->db_oid, r->rel_number, r->fork_number);
				return buf;
			}
		case NeonIORequest_UpdateCachedRelSize:
			{
				CUpdateCachedRelSizeRequest *r = &request->update_cached_rel_size;

				snprintf(buf, sizeof(buf), "UpdateCachedRelSize: req " UINT64_FORMAT " rel %u/%u/%u.%u blocks: %u",
						 r->request_id,
						 r->spc_oid, r->db_oid, r->rel_number, r->fork_number,
						 r->nblocks);
				return buf;
			}
	}
	snprintf(buf, sizeof(buf), "Unknown request type %d", (int) request->tag);
	return buf;
}


/*
 * The worker process can read / write shared buffers directly. But if smgrread() or
 * smgrwrite() is called with a private temporary buffer, we need to copy it to the
 * "bounce buffer", to make it available for the worker process.
 */
static bool
bounce_needed(void *buffer)
{
	if ((uintptr_t) buffer >= (uintptr_t) BufferBlocks &&
		(uintptr_t) buffer < (uintptr_t) BufferBlocks + NBuffers * BLCKSZ)
	{
		return false;
	}
	return true;
}

static void *
bounce_buf(void)
{
	return &communicator_shmem_ptr->backends[MyProcNumber].bounce_buffer;
}

static void *
bounce_write_if_needed(void *buffer)
{
	void	   *p;

	if (!bounce_needed(buffer))
		return buffer;

	p = bounce_buf();
	memcpy(p, buffer, BLCKSZ);
	return p;
}

int32
communicator_new_approximate_working_set_size_seconds(time_t duration, bool reset)
{
	int32		dc;

	dc = (int32) estimateSHLL(&communicator_shmem_ptr->wss_estimation, duration);
	if (reset)
		memset(communicator_shmem_ptr->wss_estimation.regs, 0, sizeof(communicator_shmem_ptr->wss_estimation.regs));
	return dc;
}

/*
 * Return an array of LfcStatsEntrys
 */
LfcStatsEntry *
communicator_new_lfc_get_stats(size_t *num_entries)
{
	LfcStatsEntry *entries;
	size_t		n = 0;
	uint64		cache_hits = 0;
	uint64		cache_misses = 0;

	for (int i = 0; i < MaxProcs; i++)
	{
		cache_hits += communicator_shmem_ptr->backends[i].cache_hits;
		cache_misses += communicator_shmem_ptr->backends[i].cache_misses;
	}

#define NUM_ENTRIES 10
	entries = palloc(sizeof(LfcStatsEntry) * NUM_ENTRIES);

	entries[n++] = (LfcStatsEntry)
	{
		"file_cache_hits", false, cache_hits
	};
	entries[n++] = (LfcStatsEntry)
	{
		"file_cache_misses", false, cache_misses
	};

	entries[n++] = (LfcStatsEntry)
	{
		"file_cache_used_pages", false,
			bcomm_cache_get_num_pages_used(my_bs)
	};

	/* TODO: these stats are exposed by the legacy LFC implementation */
#if 0
	entries[n++] = (LfcStatsEntry)
	{
		"file_cache_used", lfc_ctl == NULL,
			lfc_ctl ? lfc_ctl->used : 0
	};
	entries[n++] = (LfcStatsEntry)
	{
		"file_cache_writes", lfc_ctl == NULL,
			lfc_ctl ? lfc_ctl->writes : 0
	};
	entries[n++] = (LfcStatsEntry)
	{
		"file_cache_size", lfc_ctl == NULL,
			lfc_ctl ? lfc_ctl->size : 0
	};
	entries[n++] = (LfcStatsEntry)
	{
		"file_cache_evicted_pages", lfc_ctl == NULL,
			lfc_ctl ? lfc_ctl->evicted_pages : 0
	};
	entries[n++] = (LfcStatsEntry)
	{
		"file_cache_limit", lfc_ctl == NULL,
			lfc_ctl ? lfc_ctl->limit : 0
	};
#endif

	Assert(n <= NUM_ENTRIES);

	*num_entries = n;
	return entries;
}

/*
 * Get metrics, for the built-in metrics exporter that's part of the
 * communicator process.
 *
 * NB: This is called from a Rust tokio task inside the communicator process.
 * Acquiring lwlocks, elog(), allocating memory or anything else non-trivial
 * is strictly prohibited here!
 */
struct LfcMetrics
communicator_new_get_lfc_metrics_unsafe(void)
{
	uint64		cache_hits = 0;
	uint64		cache_misses = 0;

	struct LfcMetrics result = {
		.lfc_cache_size_limit = (int64) lfc_size_limit * 1024 * 1024,
		.lfc_used = 0, /* TODO */
		.lfc_writes = 0, /* TODO */
	};

	for (int i = 0; i < MaxProcs; i++)
	{
		cache_hits += communicator_shmem_ptr->backends[i].cache_hits;
		cache_misses += communicator_shmem_ptr->backends[i].cache_misses;
	}
	result.lfc_hits = cache_hits;
	result.lfc_misses = cache_misses;

	for (int minutes = 1; minutes <= 60; minutes++)
	{
		result.lfc_approximate_working_set_size_windows[minutes - 1] =
			communicator_new_approximate_working_set_size_seconds(minutes * 60, false);
	}

	return result;
}
