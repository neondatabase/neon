name: TPC-C like benchmark using benchbase

on:
  # uncomment to run on push for debugging your PR
  push:
    branches: [ bodobolero/benchbase ]
  schedule:
    # * is a special character in YAML so you have to quote this string
    #          ┌───────────── minute (0 - 59)
    #          │ ┌───────────── hour (0 - 23)
    #          │ │ ┌───────────── day of the month (1 - 31)
    #          │ │ │ ┌───────────── month (1 - 12 or JAN-DEC)
    #          │ │ │ │ ┌───────────── day of the week (0 - 6 or SUN-SAT)
    - cron:   '0 6 * * *' # run once a day at 6 AM UTC
  workflow_dispatch: # adds ability to run this manually

defaults:
  run:
    shell: bash -euxo pipefail {0}

concurrency:
  # Allow only one workflow globally because we need dedicated resources which only exist once
  group: benchbase-tpcc-workflow
  cancel-in-progress: false

permissions:
  contents: read

jobs:
  benchbase-tpcc:
    strategy:
      fail-fast: false # allow other variants to continue even if one fails
      matrix:
        include:
          - warehouses: 50
            max_rate: 800
            min_cu: 0.25
            max_cu: 2
        #   - warehouses: 500
        #     max_rate: 2000
        #     min_cu: 2
        #     max_cu: 8
        #   - warehouses: 1000
        #     max_rate: 2900
        #     min_cu: 2
        #     max_cu: 16
      max-parallel: 1 # we want to run each workload size sequentially to avoid noisy neighbors
    permissions:
      contents: write
      statuses: write
      id-token: write # aws-actions/configure-aws-credentials
    env:
      PG_CONFIG: /tmp/neon/pg_install/v17/bin/pg_config
      PSQL: /tmp/neon/pg_install/v17/bin/psql
      PG_17_LIB_PATH: /tmp/neon/pg_install/v17/lib
      POSTGRES_VERSION: 17
    runs-on: [ self-hosted, us-east-2, x64 ]
    timeout-minutes: 1440

    steps:
    - name: Harden the runner (Audit all outbound calls)
      uses: step-security/harden-runner@4d991eb9b905ef189e4c376166672c3f2f230481 # v2.11.0
      with:
        egress-policy: audit

    - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

    - name: Configure AWS credentials # necessary to download artefacts
      uses: aws-actions/configure-aws-credentials@e3dd6a429d7300a6a4c196c26e071d42e0343502 # v4.0.2
      with:
        aws-region: eu-central-1
        role-to-assume: ${{ vars.DEV_AWS_OIDC_ROLE_ARN }}
        role-duration-seconds: 18000 # 5 hours is currently max associated with IAM role

    - name: Download Neon artifact
      uses: ./.github/actions/download
      with:
        name: neon-${{ runner.os }}-${{ runner.arch }}-release-artifact
        path: /tmp/neon/
        prefix: latest
        aws-oidc-role-arn: ${{ vars.DEV_AWS_OIDC_ROLE_ARN }}

    - name: Create Neon Project
      id: create-neon-project-tpcc
      uses: ./.github/actions/neon-project-create
      with:
        region_id: aws-us-east-2
        postgres_version: ${{ env.POSTGRES_VERSION }}
        compute_units: '[${{ matrix.min_cu }}, ${{ matrix.max_cu }}]'
        api_key: ${{ secrets.NEON_PRODUCTION_API_KEY_4_BENCHMARKS }}
        api_host: console.neon.tech  # production (!)

    - name: Initialize Neon project
      env:
          BENCHMARK_TPCC_CONNSTR: ${{ steps.create-neon-project-tpcc.outputs.dsn }}
          PROJECT_ID: ${{ steps.create-neon-project-tpcc.outputs.project_id }}
      run: |
        echo "Initializing Neon project with project_id: ${PROJECT_ID}"
        export LD_LIBRARY_PATH=${PG_17_LIB_PATH}
        
        # Retry logic for psql connection with 1 minute sleep between attempts
        for attempt in {1..3}; do
          echo "Attempt ${attempt}/3: Creating extensions in Neon project"
          if ${PSQL} "${BENCHMARK_TPCC_CONNSTR}" -c "CREATE EXTENSION IF NOT EXISTS neon; CREATE EXTENSION IF NOT EXISTS neon_utils;"; then
            echo "Successfully created extensions"
            break
          else
            echo "Failed to create extensions on attempt ${attempt}"
            if [ ${attempt} -lt 3 ]; then
              echo "Waiting 60 seconds before retry..."
              sleep 60
            else
              echo "All attempts failed, exiting"
              exit 1
            fi
          fi
        done
        
        echo "BENCHMARK_TPCC_CONNSTR=${BENCHMARK_TPCC_CONNSTR}" >> $GITHUB_ENV

    - name: Generate BenchBase workload configuration
      env:
        WAREHOUSES: ${{ matrix.warehouses }}
        MAX_RATE: ${{ matrix.max_rate }}
      run: |
        echo "Generating BenchBase configs for warehouses: ${WAREHOUSES}, max_rate: ${MAX_RATE}"
        
        # Extract hostname and password from connection string
        # Format: postgresql://username:password@hostname/database?params (no port for Neon)
        HOSTNAME=$(echo "${BENCHMARK_TPCC_CONNSTR}" | sed -n 's|.*://[^:]*:[^@]*@\([^/]*\)/.*|\1|p')
        PASSWORD=$(echo "${BENCHMARK_TPCC_CONNSTR}" | sed -n 's|.*://[^:]*:\([^@]*\)@.*|\1|p')
        
        echo "Extracted hostname: ${HOSTNAME}"
        
        # Set up working directories in runner temp (NVMe)
        BENCHMARK_WORKDIR="${RUNNER_TEMP}/benchbase-tpcc"
        mkdir -p "${BENCHMARK_WORKDIR}"
        cd "${BENCHMARK_WORKDIR}"
        
        # Copy the generator script
        cp "${GITHUB_WORKSPACE}/test_runner/performance/benchbase_tpc_c_helpers/generate_workload_size.py" .
        
        # Generate configs and scripts
        python3 generate_workload_size.py \
          --warehouses ${WAREHOUSES} \
          --max-rate ${MAX_RATE} \
          --hostname ${HOSTNAME} \
          --password ${PASSWORD} \
          --runner-arch ${{ runner.arch }}
        
        # Fix path mismatch: move generated configs and scripts to expected locations
        mv ../configs ./configs
        mv ../scripts ./scripts
        
        # Save working directory for subsequent steps
        echo "BENCHMARK_WORKDIR=${BENCHMARK_WORKDIR}" >> $GITHUB_ENV

    - name: Prepare database (load data)
      env:
        WAREHOUSES: ${{ matrix.warehouses }}
      run: |
        cd "${BENCHMARK_WORKDIR}"
        
        echo "Loading ${WAREHOUSES} warehouses into database..."
        
        # Run the loader script (creates its own results_load directory)
        ./scripts/load_${WAREHOUSES}_warehouses.sh
        
        echo "Database loading completed"

    - name: Run TPC-C benchmark
      env:
        WAREHOUSES: ${{ matrix.warehouses }}
      run: |
        cd "${BENCHMARK_WORKDIR}"
        
        echo "Running TPC-C benchmark with ${WAREHOUSES} warehouses..."
        
        # Run the optimal rate benchmark
        ./scripts/execute_${WAREHOUSES}_warehouses_opt_rate.sh
        
        echo "Benchmark execution completed"

    - name: Process results (upload to test results database and generate diagrams)
      env:
        WAREHOUSES: ${{ matrix.warehouses }}
        MIN_CU: ${{ matrix.min_cu }}
        MAX_CU: ${{ matrix.max_cu }}
        PROJECT_ID: ${{ steps.create-neon-project-tpcc.outputs.project_id }}
        REVISION: ${{ github.sha }}
        PERF_DB_CONNSTR: ${{ secrets.PERF_TEST_RESULT_CONNSTR }}
      run: |
        cd "${BENCHMARK_WORKDIR}"
        
        echo "Creating temporary Python environment for results processing..."
        
        # Create temporary virtual environment
        python3 -m venv temp_results_env
        source temp_results_env/bin/activate
        
        # Install required packages in virtual environment
        pip install matplotlib pandas psycopg2-binary
        
        echo "Copying results processing scripts..."
        
        # Copy both processing scripts
        cp "${GITHUB_WORKSPACE}/test_runner/performance/benchbase_tpc_c_helpers/generate_diagrams.py" .
        cp "${GITHUB_WORKSPACE}/test_runner/performance/benchbase_tpc_c_helpers/upload_results_to_perf_test_results.py" .
        
        echo "Processing warmup results..."
        
        # Find and process warmup results
        WARMUP_CSV=$(find results_warmup -name "*.results.csv" -type f | head -1)
        WARMUP_JSON=$(find results_warmup -name "*.summary.json" -type f | head -1)
        
        if [ -n "$WARMUP_CSV" ] && [ -n "$WARMUP_JSON" ]; then
          echo "Generating warmup diagram from: $WARMUP_CSV"
          python generate_diagrams.py \
            --input-csv "$WARMUP_CSV" \
            --output-svg "warmup_${WAREHOUSES}_warehouses_performance.svg" \
            --title-suffix "Warmup at max TPS"
            
          echo "Uploading warmup metrics from: $WARMUP_JSON"
          python upload_results_to_perf_test_results.py \
            --summary-json "$WARMUP_JSON" \
            --results-csv "$WARMUP_CSV" \
            --run-type "warmup" \
            --min-cu "${MIN_CU}" \
            --max-cu "${MAX_CU}" \
            --project-id "${PROJECT_ID}" \
            --revision "${REVISION}" \
            --connection-string "${PERF_DB_CONNSTR}"
        else
          echo "Warning: Missing warmup results files (CSV: $WARMUP_CSV, JSON: $WARMUP_JSON)"
        fi
        
        echo "Processing optimal rate results..."
        
        # Find and process optimal rate results  
        OPTRATE_CSV=$(find results_opt_rate -name "*.results.csv" -type f | head -1)
        OPTRATE_JSON=$(find results_opt_rate -name "*.summary.json" -type f | head -1)
        
        if [ -n "$OPTRATE_CSV" ] && [ -n "$OPTRATE_JSON" ]; then
          echo "Generating optimal rate diagram from: $OPTRATE_CSV"
          python generate_diagrams.py \
            --input-csv "$OPTRATE_CSV" \
            --output-svg "benchmark_${WAREHOUSES}_warehouses_performance.svg" \
            --title-suffix "70% of max TPS"
            
          echo "Uploading optimal rate metrics from: $OPTRATE_JSON"
          python upload_results_to_perf_test_results.py \
            --summary-json "$OPTRATE_JSON" \
            --results-csv "$OPTRATE_CSV" \
            --run-type "opt-rate" \
            --min-cu "${MIN_CU}" \
            --max-cu "${MAX_CU}" \
            --project-id "${PROJECT_ID}" \
            --revision "${REVISION}" \
            --connection-string "${PERF_DB_CONNSTR}"
        else
          echo "Warning: Missing optimal rate results files (CSV: $OPTRATE_CSV, JSON: $OPTRATE_JSON)"
        fi
        
        # Deactivate and clean up virtual environment
        deactivate
        rm -rf temp_results_env
        rm upload_results_to_perf_test_results.py
        
        echo "Results processing completed and environment cleaned up"

    - name: Set date for upload
      id: set-date
      run: echo "date=$(date +%Y-%m-%d)" >> $GITHUB_OUTPUT

    - name: Configure AWS credentials # necessary to upload results
      uses: aws-actions/configure-aws-credentials@e3dd6a429d7300a6a4c196c26e071d42e0343502 # v4.0.2
      with:
        aws-region: us-east-2
        role-to-assume: ${{ vars.DEV_AWS_OIDC_ROLE_ARN }}
        role-duration-seconds: 900 # 900 is minimum value 
        
    - name: Upload benchmark results to S3
      env:
        S3_BUCKET: neon-public-benchmark-results
        S3_PREFIX: benchbase-tpc-c/${{ steps.set-date.outputs.date }}/${{ github.run_id }}/${{ matrix.warehouses }}-warehouses
      run: |
        echo "Redacting passwords from configuration files before upload..."
        
        # Mask all passwords in XML config files
        find "${BENCHMARK_WORKDIR}/configs" -name "*.xml" -type f -exec sed -i 's|<password>[^<]*</password>|<password>redacted</password>|g' {} \;
        
        echo "Uploading benchmark results to s3://${S3_BUCKET}/${S3_PREFIX}/"
        
        # Upload the entire benchmark directory recursively
        aws s3 cp --only-show-errors --recursive "${BENCHMARK_WORKDIR}" s3://${S3_BUCKET}/${S3_PREFIX}/
        
        echo "Upload completed"

    - name: Clean up benchmark working directory
      if: ${{ always() }}
      run: |
        if [ -n "${BENCHMARK_WORKDIR}" ] && [ -d "${BENCHMARK_WORKDIR}" ]; then
          echo "Cleaning up benchmark working directory: ${BENCHMARK_WORKDIR}"
          rm -rf "${BENCHMARK_WORKDIR}"
          echo "Cleanup completed"
        else
          echo "No benchmark working directory to clean up"
        fi

    - name: Delete Neon Project
      if: ${{ always() }}
      uses: ./.github/actions/neon-project-delete
      with:
        project_id: ${{ steps.create-neon-project-tpcc.outputs.project_id }}
        api_key: ${{ secrets.NEON_PRODUCTION_API_KEY_4_BENCHMARKS }} 
        api_host: console.neon.tech  # production (!)