name: 'Run python test'
description: 'Runs a Neon python test set, performing all the required preparations before'

inputs:
  build_type:
    description: 'Type of Rust (neon) and C (postgres) builds. Must be "release" or "debug", or "remote" for the remote cluster'
    required: true
  test_selection:
    description: 'A python test suite to run'
    required: true
  extra_params:
    description: 'Arbitrary parameters to pytest. For example "-s" to prevent capturing stdout/stderr'
    required: false
    default: ''
  needs_postgres_source:
    description: 'Set to true if the test suite requires postgres source checked out'
    required: false
    default: 'false'
  run_in_parallel:
    description: 'Whether to run tests in parallel'
    required: false
    default: 'true'
  save_perf_report:
    description: 'Whether to upload the performance report, if true PERF_TEST_RESULT_CONNSTR env variable should be set'
    required: false
    default: 'false'
  run_with_real_s3:
    description: 'Whether to pass real s3 credentials to the test suite'
    required: false
    default: 'false'
  real_s3_bucket:
    description: 'Bucket name for real s3 tests'
    required: false
    default: ''
  real_s3_region:
    description: 'Region name for real s3 tests'
    required: false
    default: ''
  real_s3_access_key_id:
    description: 'Access key id'
    required: false
    default: ''
  real_s3_secret_access_key:
    description: 'Secret access key'
    required: false
    default: ''

runs:
  using: "composite"
  steps:
    - name: Get Neon artifact
      if: inputs.build_type != 'remote'
      uses: ./.github/actions/download
      with:
        name: neon-${{ runner.os }}-${{ inputs.build_type }}-artifact
        path: /tmp/neon

    - name: Download Neon binaries for the previous release
      if: inputs.build_type != 'remote'
      uses: ./.github/actions/download
      with:
        name: neon-${{ runner.os }}-${{ inputs.build_type }}-artifact
        path: /tmp/neon-previous
        prefix: latest

    - name: Download compatibility snapshot for Postgres 14
      if: inputs.build_type != 'remote'
      uses: ./.github/actions/download
      with:
        name: compatibility-snapshot-${{ inputs.build_type }}-pg15
        path: /tmp/compatibility_snapshot_pg15
        prefix: latest

    - name: Checkout
      if: inputs.needs_postgres_source == 'true'
      uses: actions/checkout@v3
      with:
        submodules: true
        fetch-depth: 1

    - name: Cache poetry deps
      id: cache_poetry
      uses: actions/cache@v3
      with:
        path: ~/.cache/pypoetry/virtualenvs
        key: v1-${{ runner.os }}-python-deps-${{ hashFiles('poetry.lock') }}

    - name: Install Python deps
      shell: bash -euxo pipefail {0}
      run: ./scripts/pysync

    - name: Run pytest
      env:
        NEON_BIN: /tmp/neon/bin
        COMPATIBILITY_NEON_BIN: /tmp/neon-previous/bin
        COMPATIBILITY_POSTGRES_DISTRIB_DIR: /tmp/neon-previous/pg_install
        TEST_OUTPUT: /tmp/test_output
        BUILD_TYPE: ${{ inputs.build_type }}
        AWS_ACCESS_KEY_ID: ${{ inputs.real_s3_access_key_id }}
        AWS_SECRET_ACCESS_KEY: ${{ inputs.real_s3_secret_access_key }}
        COMPATIBILITY_SNAPSHOT_DIR: /tmp/compatibility_snapshot_pg15
        ALLOW_BACKWARD_COMPATIBILITY_BREAKAGE: contains(github.event.pull_request.labels.*.name, 'backward compatibility breakage')
        ALLOW_FORWARD_COMPATIBILITY_BREAKAGE: contains(github.event.pull_request.labels.*.name, 'forward compatibility breakage')
      shell: bash -euxo pipefail {0}
      run: |
        # PLATFORM will be embedded in the perf test report
        # and it is needed to distinguish different environments
        export PLATFORM=${PLATFORM:-github-actions-selfhosted}
        export POSTGRES_DISTRIB_DIR=${POSTGRES_DISTRIB_DIR:-/tmp/neon/pg_install}
        export DEFAULT_PG_VERSION=${DEFAULT_PG_VERSION:-15}

        if [ "${BUILD_TYPE}" = "remote" ]; then
          export REMOTE_ENV=1
        fi

        PERF_REPORT_DIR="$(realpath test_runner/perf-report-local)"
        rm -rf $PERF_REPORT_DIR

        TEST_SELECTION="test_runner/${{ inputs.test_selection }}"
        EXTRA_PARAMS="${{ inputs.extra_params }}"
        if [ -z "$TEST_SELECTION" ]; then
          echo "test_selection must be set"
          exit 1
        fi
        if [[ "${{ inputs.run_in_parallel }}" == "true" ]]; then
          # -n4 uses four processes to run tests via pytest-xdist
          EXTRA_PARAMS="-n4 $EXTRA_PARAMS"

          # --dist=loadgroup points tests marked with @pytest.mark.xdist_group
          # to the same worker to make @pytest.mark.order work with xdist
          EXTRA_PARAMS="--dist=loadgroup $EXTRA_PARAMS"
        fi

        if [[ "${{ inputs.run_with_real_s3 }}" == "true" ]]; then
          echo "REAL S3 ENABLED"
          export ENABLE_REAL_S3_REMOTE_STORAGE=nonempty
          export REMOTE_STORAGE_S3_BUCKET=${{ inputs.real_s3_bucket }}
          export REMOTE_STORAGE_S3_REGION=${{ inputs.real_s3_region }}
        fi

        if [[ "${{ inputs.save_perf_report }}" == "true" ]]; then
          mkdir -p "$PERF_REPORT_DIR"
          EXTRA_PARAMS="--out-dir $PERF_REPORT_DIR $EXTRA_PARAMS"
        fi

        if [[ "${{ inputs.build_type }}" == "debug" ]]; then
          cov_prefix=(scripts/coverage "--profraw-prefix=$GITHUB_JOB" --dir=/tmp/coverage run)
        elif [[ "${{ inputs.build_type }}" == "release" ]]; then
          cov_prefix=()
        else
          cov_prefix=()
        fi

        # Wake up the cluster if we use remote neon instance
        if [ "${{ inputs.build_type }}" = "remote" ] && [ -n "${BENCHMARK_CONNSTR}" ]; then
          ${POSTGRES_DISTRIB_DIR}/v${DEFAULT_PG_VERSION}/bin/psql ${BENCHMARK_CONNSTR} -c "SELECT version();"
        fi

        # Run the tests.
        #
        # The junit.xml file allows CI tools to display more fine-grained test information
        # in its "Tests" tab in the results page.
        # --verbose prints name of each test (helpful when there are
        # multiple tests in one file)
        # -rA prints summary in the end
        # -s is not used to prevent pytest from capturing output, because tests are running
        # in parallel and logs are mixed between different tests
        #
        mkdir -p $TEST_OUTPUT/allure/results
        "${cov_prefix[@]}" ./scripts/pytest \
          --junitxml=$TEST_OUTPUT/junit.xml \
          --alluredir=$TEST_OUTPUT/allure/results \
          --tb=short \
          --verbose \
          -rA $TEST_SELECTION $EXTRA_PARAMS

        if [[ "${{ inputs.save_perf_report }}" == "true" ]]; then
          export REPORT_FROM="$PERF_REPORT_DIR"
          export REPORT_TO="$PLATFORM"
          scripts/generate_and_push_perf_report.sh
        fi

    - name: Upload compatibility snapshot for Postgres 15
      if: github.ref_name == 'release'
      uses: ./.github/actions/upload
      with:
        name: compatibility-snapshot-${{ inputs.build_type }}-pg15-${{ github.run_id }}
        # The path includes a test name (test_create_snapshot) and directory that the test creates (compatibility_snapshot_pg15), keep the path in sync with the test
        path: /tmp/test_output/test_create_snapshot/compatibility_snapshot_pg15/
        prefix: latest

    - name: Create Allure report
      if: success() || failure()
      uses: ./.github/actions/allure-report
      with:
        action: store
        build_type: ${{ inputs.build_type }}
        test_selection: ${{ inputs.test_selection }}
